launcher:
  launcher: bash
  torchrun: true
  copy_code: false

  name: memory
  overwrite: true
  log_dir: $HOME/memory_debug

  script: apps.memory.train

  slurm:
    # partition: scavenge
    mem: 16G
    nodes: 1
    nb_gpus: 2        # use torchrun, or slurm, for multi-gpu runs
    time: 10          # job time in minutes
    signal_time: 60   # alert time in seconds

run_config:
  cluster:
    compile_model: true

  data:
    batch_size: 128
    seq_len: 129
    asynchronous: true

    n_data: 1000
    key: biographies
    data_dir: $HOME/data/memory
    save_dir: $HOME/data/temporary
    tokenizer:
      name: byte
      special_tokens:
        <|user|>: 0
        <|assistant|>: 1
        <|database|>: 2
        <|eod|>: 3
      # name: tiktoken
      # path: $HOME/tokenizer/cl100k_base.bpe
      # configuration:
      #   nb_special_tokens: 21
      #   pattern: '(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}++|\\p{N}{1,3}+| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*+|\\s++$|\\s*[\\r\\n]|\\s+(?!\\S)|\\s'

  model:
    implementation: transformer
    flex_attention: true
    vocab_size: 300
    emb_dim: 64
    nb_layers: 2
    block:
      nb_heads: 2
      hidden_dim: 256

  optim:
    steps: 10000
    max_steps: 10000
    lr: 1e-4
    weight_decay: 0.1
    fused: false
    warmup: 15
    lr_min_ratio: 0

  orchestration:
    utils:
      seed: 100

    checkpoint:
      period: 20
      nb_kept: 3

    logging:
      period: 1
      level: INFO
    # wandb:
    #   active: true

    profiler:
      active: false
      heavy: true
      wait: 1
      steps: 20

  # eval:
  #   period: 0
  #   dataset_dir: /checkpoint/$USER/datasets/eval
  #   tasks: piqa
  #   generator:
  #     max_tokens: 2048
      # dtype: fp32

  # evaluation:
  #   period: 15
  #   asynchronous: false
  #   data:
  #     path: $HOME/data/composition/base/testset.h5
  #     n_data: 1_000
  #     batch_size: 16
