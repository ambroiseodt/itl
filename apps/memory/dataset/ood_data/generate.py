# This source code is licensed under the terms specified in the `LICENSE` file.
"""
Generate a list of entities with random attributes.

### Notes
The created entities cannnot be generated by generate.py as the names are taken from first_names_ood.txt.

@ 2025, Meta
"""

import json
import logging
import os
import random
import re
from itertools import product
from pathlib import Path, PosixPath

from jinja2 import Template

# relative paths
TEMPLATE_DIR = Path(__file__).parents[1].resolve() / "templates"
ATOM_DIR = Path(__file__).parents[1].resolve() / "atoms"

# default variables
SEED = 42
DATA_DIR = Path().home() / "ood_data" / "memory"
logger = logging.getLogger("nanollama")


def generate_people(save_dir: str = DATA_DIR, seed: int = SEED) -> None:
    """
    Generate a list of people with random attributes.
    """
    # parse arguments
    save_dir: PosixPath = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)

    # recover atom data
    with open(ATOM_DIR / "first_names_ood.txt") as f:
        first_names = f.read().splitlines()
    with open(ATOM_DIR / "last_names.txt") as f:
        last_names = f.read().splitlines()
    with open(ATOM_DIR / "cities.txt") as f:
        cities = f.read().splitlines()
    with open(ATOM_DIR / "countries.txt") as f:
        countries = f.read().splitlines()
    with open(ATOM_DIR / "occupations.txt") as f:
        occupations = f.read().splitlines()

    # shuffle people ordering
    random.seed(seed)
    all_pairs = list(product(first_names, last_names))
    random.shuffle(all_pairs)

    # generate all unique combinations of first and last names and save to file
    save_file = save_dir / "people.jsonl"
    with open(save_file, "w") as f:
        for first_name, last_name in all_pairs:
            entity = {
                "name": f"{first_name} {last_name}",
                "birth_date": f"{random.randint(1, 28)}/{random.randint(1, 12)}/{random.randint(1950, 2000)}",
                "birth_place": f"{random.choice(countries)}",
                "current_address": f"{random.choice(cities)}",
                "occupation": random.choice(occupations),
            }
            print(json.dumps(entity), file=f, flush=True)


def collect_people(save_dir: str = DATA_DIR, num: int = float("inf")) -> list[dict[str, str]]:
    """
    Collect a list of people with random attributes.

    ### Parameters
    - num: Number of people to collect (by default, it collect everyone).
    """
    # parse arguments
    save_dir: PosixPath = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)

    save_file = save_dir / "people.jsonl"
    people = []
    with open(save_file) as f:
        while len(people) < num:
            line = f.readline()
            if not line:
                break
            people.append(json.loads(line))
    return people


def generate_biographies(save_dir: str = DATA_DIR, num: int = float("inf")) -> None:
    """
    Generate a list of biographies of people.
    """
    # parse arguments
    save_dir: PosixPath = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)

    # recover templates
    templates: list[Template] = []
    for file in (TEMPLATE_DIR).glob("bio*.j2"):
        with open(file) as f:
            templates.append(Template(f.read()))

    # write biographies to file
    for i, people in enumerate(collect_people(num=num)):
        person_dir = save_dir / f"person{i + 1}"
        person_dir.mkdir(parents=True, exist_ok=True)

        with open(person_dir / "biographies.jsonl", "w") as f:
            for template in templates:
                biography = template.render(**people)
                dialog = [{"source": "assistant", "content": biography}]
                print(
                    json.dumps({"dialog": dialog, "people_id": i}),
                    file=f,
                    flush=True,
                )


def generate_qa(save_dir: str = DATA_DIR, num: int = float("inf"), tooluse: bool = False) -> None:
    """
    Generate a list of questions about people biographies.
    """
    # parse arguments
    save_dir: PosixPath = Path(save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)

    # recover templates
    templates: list[list[dict[str, Template]]] = []
    identifier = "qa" + ("tool" if tooluse else "")
    # regular expression to match special tokens
    pattern = re.compile(r"^<\|(\w+)\|>(.*)$")

    for file in (TEMPLATE_DIR).glob(f"{identifier}?.j2"):
        with open(file) as f:
            dialog = f.read().splitlines()
            out, source, buffer = [], None, []
            for message in dialog:
                match = pattern.match(message)
                if match:
                    if source is not None:
                        # flush bufferized text
                        out.append({"source": source, "content": Template("\n".join(buffer))})

                    # start a new message
                    source, message = match.groups()
                    source = source.lower()
                    buffer = [message.strip()]
                else:
                    # accumulate lines to the buffer
                    buffer.append(message.strip())
            # empty the buffer
            if source is not None:
                out.append({"source": source, "content": Template("\n".join(buffer))})
            templates.append(out)

    # Create a directory for each person
    for i, people in enumerate(collect_people(num=num)):
        person_dir = save_dir / f"person{i + 1}"
        person_dir.mkdir(parents=True, exist_ok=True)

        with open(person_dir / f"{identifier}.jsonl", "w") as f:
            for dialog in templates:
                out, answer = [], None
                for message in dialog:
                    content = message["content"].render(**people)
                    if message["source"] == "answer":
                        answer = content
                    else:
                        out.append(message | {"content": content})
                print(json.dumps({"dialog": out, "people_id": i, "answer": answer}), file=f, flush=True)


def build_data(n_data: int, key: str, save_dir: str, data_dir: str = DATA_DIR) -> None:
    """
    Build a dataset folder by merging person data.

    ### Parameters:
    - num: number of people to merge into the dataset
    - key: key indicating which file to copy (e.g., 'qa', 'biographies', 'qatool').
    - save_dir: directory where the files should be copied to.
    - data_dir: directory where the data is stored.
    """
    # Ensure the target directory exists
    save_dir: PosixPath = Path(os.path.expandvars(save_dir))
    save_dir.mkdir(parents=True, exist_ok=True)

    # Iterate over the specified number of directories
    data_dir: PosixPath = Path(os.path.expandvars(data_dir))
    for i in range(n_data):
        # Define the source file path
        source_file = data_dir / f"person{i + 1}/{key}.jsonl"

        # Define the target file path
        target_file = save_dir / f"chunk.{i}.jsonl"

        # Copy the file if it exists
        if not source_file.exists():
            logger.debug(f"File {source_file} does not exist and was skipped.")
        elif target_file.exists():
            logger.debug(f"File {target_file} already exists and was skipped.")
        else:
            os.symlink(source_file, target_file)
    logger.debug(f"Dataset created at {save_dir}, from {n_data} people.")


if __name__ == "__main__":
    import fire

    logging.basicConfig(level=logging.INFO)

    fire.Fire(
        {
            "people": generate_people,
            "biographies": generate_biographies,
            "qa": generate_qa,
            "build": build_data,
        }
    )
