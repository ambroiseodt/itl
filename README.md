# üõ†Ô∏è Provable Benefits of In-Tool Learning (ITL)
[![Arxiv](https://img.shields.io/badge/arXiv-2508.20755-b5212f?logo=arxiv)](https://arxiv.org/pdf/2508.20755)
[![License](https://img.shields.io/badge/License-CC--BY--NC-blue.svg)](https://creativecommons.org/licenses/by-nc/4.0/legalcode.en)

**[Sam Houliston*](https://www.linkedin.com/in/sam-houliston-47364524a/?originalSubdomain=uk), [Ambroise Odonnat*](https://ambroiseodt.github.io/), [Charles Arnal*](https://charlesarnal.github.io/), [Vivien Cabannes*](https://viviencabannes.github.io/)**. ***Equal contribution**.

Our codebase provides utilities to train and study large language models from a memory and generalization perspective. It relies mainly on PyTorch primitives, instead of any high-level LLM libraries, allowing researchers and practitioners to easily prototype and modify. It can be used to reproduce the experiments of our paper [Provable Benefits of In-Tool Learning for Large Language Models](https://arxiv.org/pdf/2508.20755), in which we show that tool-augmented workflows are not only practical, but also provably more scalable.

<p align="center">
 <img src="overview.svg" width="100%"/>
</p>

- üõ†Ô∏è In-tool learning: learning to use a tool (e.g., a calculator or a request to a database) to solve a problem,
- üèãüèΩ In-weight learning: memorizing the solution to a problem within the model's weights.

## Overview
Our codebase is structured as follows:

```
üõ†Ô∏è itl
‚î£ üìÇsrc # Core library NanoLlama
‚îÉ ‚î£ üìÇnanollama
‚îÉ   ‚î£ üìÇagent
‚îÉ   ‚î£ üìÇdata
‚îÉ   ‚î£ üìÇinference
‚îÉ   ‚î£ üìÇmodel
‚îÉ   ‚î£ üìÇmonitor
‚îÉ   ‚î£ üìÇvisualization
‚îÉ   ‚î£ üìÑ__init__.py
‚îÉ   ‚î£ üìÑdistributed.py
‚îÉ   ‚î£ üìÑlauncher.py
‚îÉ   ‚î£ üìÑoptim.py
‚îÉ   ‚î£ üìÑtokenizer.py
‚îÉ   ‚îó üìÑutils.py
‚î£ üìÇtest # Unit tests
‚îó üìÇapps # In-tool learning with Nanollama
  ‚î£ üìÇmemory # Controlled study of memory load
  ‚îÉ ‚î£ üìÇcompressibility
  ‚îÉ ‚î£ üìÇconfigs
  ‚îÉ ‚î£ üìÇdatasets
  ‚îÉ ‚î£ üìÇgeneralization
  ‚îÉ ‚î£ üìÇplots
  ‚îÉ ‚î£ üìÇscripts
  ‚îÉ ‚î£ üìÑ__init__.py
  ‚îÉ ‚î£ üìÑREADME.md
  ‚îÉ ‚î£ üìÑargs.py
  ‚îÉ ‚î£ üìÑeval.py
  ‚îÉ ‚î£ üìÑlocal_grid.py
  ‚îÉ ‚î£ üìÑprompt_loader.py
  ‚îÉ ‚îó üìÑtrain.py
  ‚î£ üìÇlarge_scale # Large-scale experiments
  ‚îÉ ‚î£ üìÇdata
  ‚îÉ ‚î£ üìÇtraining
  ‚îÉ ‚î£ üìÇevaluation
  ‚îÉ ‚î£ üìÇplots
  ‚îÉ ‚î£ üìÑ__init__.py
  ‚îó ‚îó üìÑREADME.md

```

The folder ```src/nanollama``` contains the most reusable components, which can be put together in the ```apps``` folder for various applications. The code in ```apps/memory``` can be used to study the memory load of in-tool learning in a controlled setting and the code in ```apps/large_scale``` can be used to study in-tool learning at large scale.

## Getting started
We provide below the instructions to install the library and start launching experiments.
> [!NOTE]
> LLM libraries such as [datasets](https://github.com/huggingface/datasets), [transformers](https://github.com/huggingface/transformers), [trl](https://github.com/huggingface/trl), or [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) are subject to frequent changes which might impact the behavior of the codebase. In case of issues, we advise users to use previous versions of the packages.

### Installation
The code runs Python 3.10+.
Here are some installation instructions:
- Install [miniconda](https://docs.conda.io/projects/miniconda/en/latest/). Follow the instructions online, most likely you will execute the following commands.
```bash
curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash ~/Miniconda3-latest-Linux-x86_64.sh
source ~/.bashrc
```
- Install Python in a new conda environment (be mindful to install a Python version compatible with Pytorch):
```bash
conda create -n llm python==3.12
conda activate llm
```

4. Install this repo (be mindful to install a Pytorch version compatible with your CUDA driver; use `nvidia-smi` to check your CUDA driver)
```bash
git clone <repo url>
cd <repo path>
pip install -e .
```
Optional dependencies, e.g. the LLM ones, can be added by swapping the previous command for the following one:
```bash
pip install -e ".[llm]"
```
More details are given in the README files of the apps folders.

### Using HuggingFace pretrained models
Some models are gated, e.g., the Llama ones, and users should request the access and login to the huggingface hub to use them in the scripts.
See https://huggingface.co/docs/hub/en/models-gated for more information.

### Development
To verify the your installation, run unit tests with the following command at the root of this repository
```bash
python -m unittest
```
It should return ```OK```.

### Launching jobs
Our codebase supports launching jobs with and without Slurm. See ```apps/memory/README.md``` for more details.

## Reproducing our experiments
Instructions to reproduce the experiments in our [paper](https://arxiv.org/pdf/2508.20755) can be found in [apps/memory/README](apps/memory/README.md) and [apps/finetuning/README](apps/finetuning/README.md).

## Acknowledgments
This repository builds heavily on [lingua](https://github.com/facebookresearch/lingua) and [pal](https://github.com/facebookresearch/pal) which provide easy-to-use code to train and play with LLMs.

## License
The codebase is licensed under the [CC BY-NC 4.0 License](LICENSE.md).

## Citation
If you find this repository useful, please consider giving a star ‚≠ê, and citing us as:
```
@misc{in_tool_learning,
  author = {Sam Houliston* and Ambroise Odonnat* and Charles Arnal* and Vivien Cabannes*},
  title = {{Provable Benefits of In-Tool Learning for Large Language Models}},
  url = {https://arxiv.org/pdf/2508.20755},
  year = {2025}
}
```
