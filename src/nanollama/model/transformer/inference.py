# This source code is licensed under the terms specified in the `LICENSE` file.
"""
Utilities for inference and training with transformers.

For compilation reason, we want to distinguish between pretraining, prefilling and token generation.

@ 2025, Ambroise Odonnat

### Notes
There are many variants of attention implementation for fast inference.
Ultimately, the best implementation depends on your usecase.
Our implementation trades performance for simplicity.
"""

from logging import getLogger
from types import TracebackType

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch import Tensor

from ...distributed import get_raw_model
from .architecture import KVCache, Transformer, TransformerBlock

logger = getLogger("nanollama")

# ------------------------------------------------------------------------------
# Sampling
# ------------------------------------------------------------------------------


def sample_from_logits(logits: Tensor, **kwargs) -> Tensor:
    """
    Sample from logits.

    ### Parameters
    - logits: logits tensor.
    - kwargs: additional arguments for sampling strategy.

    ### Notes
    - TODO: implement various sampling strategy, e.g., temperature, top-k, etc.
    """
    return logits.argmax(dim=-1)


# ------------------------------------------------------------------------------
# Pretraining
# ------------------------------------------------------------------------------


def pretrain(model: Transformer, x: Tensor, y: Tensor, loss_mask: Tensor = None) -> Tensor:
    """
    Pretraining logic.

    ### Parameters
    - model: transformer model to be pretrained.
    - x: input tensor.
    - y: target tensor.
    - loss_mask: mask for the cross-entropy loss.

    ### Returns
    - loss: cross-entropy loss value.
    """
    # forward propagation
    preds = model(x)
    vocab_size = preds.size(-1)
    if loss_mask is not None:
        # loss on the tokens that need to be generated by the LLM
        preds = preds[loss_mask]
        y = y[loss_mask]
    preds = preds.reshape(-1, vocab_size)
    y = y.reshape(-1)
    return F.cross_entropy(preds, y)


# ------------------------------------------------------------------------------
# Token Generation
# ------------------------------------------------------------------------------


class InferenceContext:
    def __init__(self, model: nn.Module, batch_size: int, seq_len: int = None):
        # alias
        self.model = model
        self.raw_model: Transformer = get_raw_model(model)
        seq_len = seq_len or self.raw_model.seq_len
        self.cache_shape = (batch_size, self.raw_model.nb_kv_heads, seq_len, self.raw_model.head_dim)

    def __enter__(self) -> "InferenceContext":
        logger.info("Entering inference context. Setting up KV caches.")
        self.model.eval()
        device, dtype = self.raw_model.device, self.raw_model.dtype
        for layer in self.raw_model.layers:
            layer: TransformerBlock
            layer.attn.kv_cache = KVCache(self.cache_shape, device=device, dtype=dtype)
        return self

    def __exit__(self, exc: type[BaseException], value: BaseException, tb: TracebackType):
        logger.info("Exiting inference context. Deleting KV caches.")
        for layer in self.model.layers:
            layer: TransformerBlock
            layer.attn.kv_cache = None
        self.model.train()


@torch.inference_mode()
def generate(model: Transformer, x: Tensor, **kwargs) -> Tensor:
    """
    Generate tokens from a transformer model.

    ### Parameters
    - model: transformer model to generate tokens from.
    - x: input tensor.
    - kwargs: additional arguments for sampling first tokens.

    ### Returns
    - tokens: generated tokens.
    """
    logit = model(x)
    return sample_from_logits(logit, **kwargs)


# ------------------------------------------------------------------------------
# Prefilling
# ------------------------------------------------------------------------------


@torch.inference_mode()
def prefill(model: nn.Module, x: Tensor, **kwargs) -> Tensor:
    """
    Prefill caches based on prompts.

    ### Parameters
    - model: transformer model to prefill.
    - x: input tensor.
    - kwargs: additional arguments for sampling first tokens.
    """
    raw_model: Transformer = get_raw_model(model)
    for layer in raw_model.layers:
        layer: TransformerBlock
        layer.attn.kv_cache.reset()
    logits = model(x)
    # only sample the last token
    logit = logits[:, -1:]
    return sample_from_logits(logit, **kwargs)
