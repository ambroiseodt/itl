# This source code is licensed under the terms specified in the `LICENSE` file.
"""
Utilities for inference and training with transformers.

For compilation reason, we want to distinguish between pretraining, prefilling and token generation.

@ 2025, Meta
"""

import torch
import torch.nn.functional as F
from torch import Tensor
from torch.nn.attention.flex_attention import BlockMask, create_block_mask

from . import architecture as implementation
from .architecture import Transformer

# COMPILE = True

# ------------------------------------------------------------------------------
# Pretraining
# ------------------------------------------------------------------------------


def _causal_mask(b: int, h: int, q_idx: int, kv_idx: int) -> bool:
    return q_idx >= kv_idx


def build_causal_mask(seq_len: int, device: torch.device) -> BlockMask:
    if implementation.FLEX_ATTENTION:
        return create_block_mask(_causal_mask, None, None, seq_len, seq_len, device=device)
    else:
        return None


def pretrain(model: Transformer, x: Tensor, y: Tensor, mask: BlockMask, loss_mask: Tensor = None) -> Tensor:
    """
    Pretraining logic.

    ### Parameters
    - model: transformer model to be pretrained.
    - x: input tensor.
    - y: target tensor.
    - mask: Attention mask to apply to the predictions.
    - loss_mask: mask for the cross-entropy loss.

    ### Returns
    - loss: cross-entropy loss value.
    """
    preds = model(x, mask=mask)
    vocab_size = preds.size(-1)
    if loss_mask is not None:
        # loss on the tokens that need to be generated by the LLM
        preds = preds[loss_mask]
        y = y[loss_mask]
    preds = preds.reshape(-1, vocab_size)
    y = y.reshape(-1)
    return F.cross_entropy(preds, y, weight=loss_mask)


# if COMPILE:
#     pretrain = torch.compile(_pretrain)
# else:
#     pretrain = _pretrain


# ------------------------------------------------------------------------------
# Prefilling
# ------------------------------------------------------------------------------

# TODO: prefilling
# TODO: generate_token
# TODO: logic set cache, prefill, generate, delete cache
