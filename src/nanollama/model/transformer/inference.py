# This source code is licensed under the terms specified in the `LICENSE` file.
"""
Utilities for inference and training with transformers.

For compilation reason, we want to distinguish between pretraining, prefilling and token generation.

@ 2025, Meta
"""

import torch
import torch.nn.functional as F
from torch import Tensor
from torch.nn.attention.flex_attention import BlockMask, create_block_mask

from . import architecture as implementation
from .architecture import Transformer

# ------------------------------------------------------------------------------
# Pretraining
# ------------------------------------------------------------------------------


def _causal_mask(b: int, h: int, q_idx: int, kv_idx: int) -> bool:
    return q_idx >= kv_idx


def build_pretrain_mask(seq_len: int, device: torch.device) -> BlockMask:
    """
    Build a causal mask for pretraining.

    ### Parameters
    - seq_len: sequence length.
    - device: device to use.

    ### Returns
    - mask: causal mask.
    """
    if implementation.FLEX_ATTENTION:
        return create_block_mask(_causal_mask, None, None, seq_len, seq_len, device=device)
    else:
        return None


def pretrain(model: Transformer, x: Tensor, y: Tensor, mask: BlockMask, loss_mask: Tensor = None) -> Tensor:
    """
    Pretraining logic.

    ### Parameters
    - model: transformer model to be pretrained.
    - x: input tensor.
    - y: target tensor.
    - mask: Attention mask to apply to the predictions.
    - loss_mask: mask for the cross-entropy loss.

    ### Returns
    - loss: cross-entropy loss value.
    """
    # forward propagation
    preds = model(x, mask=mask)
    vocab_size = preds.size(-1)
    if loss_mask is not None:
        # loss on the tokens that need to be generated by the LLM
        preds = preds[loss_mask]
        y = y[loss_mask]
    preds = preds.reshape(-1, vocab_size)
    y = y.reshape(-1)
    return F.cross_entropy(preds, y)


# ------------------------------------------------------------------------------
# Prefilling
# ------------------------------------------------------------------------------


def setup_caches() -> None:
    ...
    # setup cache for inference


def prefill() -> Tensor:
    ...
    # prefill KV cache from prompts


# ------------------------------------------------------------------------------
# Token Generation
# ------------------------------------------------------------------------------


def generate() -> Tensor:
    ...
    # generate tokens one by one


def delete_caches() -> None:
    ...
    # delete cache to switch from inference back to training
